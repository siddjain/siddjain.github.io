---
layout: mathjax-post
title:  "Quantum Computing Notes"
date:   2020-12-24 14:54:14 -0700
categories: quantum-computing
description: notes on quantum computing
---
= Quantum Computing Notes
Siddharth Jain <siddjain@live.com>
:revdate: 2020-12-24 10:30:00 -0800
:doctype: article
:stem: latexmath
:eqnums: all
:xrefstyle: short

== The Probability Amplitude Vector or the Wave Function

A big hurdle in understanding quantum computing is the obnoxious notation that really gets in the way esp. when writing equations on paper.
Anyway begin by recalling that given a system of _n_ qubits, the state of the system is represented by a _probability amplitude vector_
aka _the wave function_ aka _the state vector_ (all 3 are the same thing) stem:[\Psi] of length **2^n^**.
_It is nothing but the joint probability distribution of the 2^n^ states that the qubit register can end up in when measured_.
When the qubit register is _measured_ all the qubits will collapse into `0` s and `1` s giving
a binary number between 0 and 2^n^-1.
_Different measurements will yield different values according to the probability amplitude vector_. 
E.g., with _n=3_, in a random measurement we will get `010` (or the number `2` expressed in decimal).
The _i-th_ component of the _probability amplitude vector_ (e.g., _i=2_ taking example above)
stem:[\Psi] is a complex number, the modulus-square of which will give the probability that when the qubit register is _measured_ it will end up in
the _i-th_ _computational basis state_ (i.e., will have the numerical value _i_). 

Example: consider _n = 3_ qubits. There are 2^3^=8 pure states which can be ordered like so:

----
[000, 001, 010, 011, 100, 101, 110, 111]
----

We can call these as states 0, 1, 2, 3, 4, 5, 6, 7, 8. i.e., _i = {0, 1, 2, 3, 4, 5, 6, 7, 8}_.

The _probability amplitude vector_ is a vector of complex numbers of length 2^3^=8 _such that_ following equation holds at all times:

[latexmath#law-of-prob]
.Law of probability
++++
\begin{equation}
\sum_{i=0}^{i=2^n} || \Psi_i || ^ 2 = 1
\end{equation}
++++

which is saying the sum of probabilities should equal 1. The LHS is also known variously as the:

* the _L^2^_ norm of the vector
* the squared length of the vector
* inner product of the vector with itself

== Quantum Circuits as Unitary Matrix Computations

A quantum circuit can be expressed as a series of matrix transforms i.e., a series of matrix multiplications.
Each matrix transform is such that the transformed _probability amplitude vector_ should obey the law of probability (<<law-of-prob>>)
or equivalently _the length of the input vector should remain the same after the transformation by the matrix_.
Mathematically this means that each matrix has to be _unitary_. The definition of a _unitary_ matrix is that its _adjoint_ is
equal to its inverse. The _adjoint_ of a matrix is defined as its _complex-conjugate transpose_ i.e., first we 
compute the complex-conjugate of the entries and then take the transpose (or vice-versa, we get the same result and order does not matter here).

Given a unitary matrix stem:[U] the _ij-th_ entry in this matrix is nothing but the complex probability amplitude that if the qubits start in
the _j-th_ state, they will end up in _i-th_ state after _measurement_. The complex probability amplitude has to be mod-squared to get the
actual probability.

== Unitary Matrices: Deeper Dive

A quantum gate has to preserve the "`total probability`" of the input wave function i.e., the total prob. should equal to stem:[1] after the
operation of the gate. The total probability is nothing but the length of the vector. _So the matrix corresponding to the gate has to preserve lengths._
Let stem:[\textbf{u}] be a vector and stem:[U] be matrix corresponding to a quantum gate acting on this vector. The new vector is given by
stem:[\textbf{v} = U\textbf{u}]. The length of the new vector is inner product of the vector with itself by definition. Let's calculate it:

[stem]
++++
\begin{equation}
|| \textbf{v} ||^2 = \textbf{v}^\dagger \textbf{v} = (U\textbf{u})^\dagger (U\textbf{u}) = \textbf{u}^\dagger U^\dagger U \textbf{u}
\end{equation}
++++

For this to equal length of the input vector stem:[\textbf{u}^\dagger \textbf{u}] we require:

[stem]
++++
\begin{equation}
U^\dagger U = I
\end{equation}
++++

The constraint stem:[U U ^\dagger = I] can be derived similarly by computing the inner product as stem:[\textbf{v} \textbf{v}^\dagger] instead of
stem:[\textbf{v}^\dagger \textbf{v}]. In summary we require:

[stem]
++++
\begin{equation}
U^\dagger U = U U ^\dagger = I
\end{equation}
++++

Its easier to prove than I thought. _Note that in order to prove this nowhere did we use the Schrodinger equation._
_The only thing we used is the requirement that probability must be conserved and an assumption that the evolution of the wave function can be described by a linear map (a matrix)._
What is the Schrodinger equation about then? The Schrodinger equation tells us the form of the unitary matrix and relates it to the Hamiltonian of the system.
That is what it brings to the table.

We have seen that unitary matrices preserve lengths. What about angles? As exercise prove that unitary matrices preserve angles (i.e., inner products)
as well. See Nielsen and Chuang equation 2.36 p. 71. 

_So a unitary matrix preserves lengths and even angles (inner products) but note that it does not have to be a rotation matrix.
Rotation matrices are even more special. They are a special case of unitary matrices containing only real values with
the additional constraint that the determinant has to be stem:[+1]. stem:[-1] corresponds to a reflection matrix. A reflection is not the
same as rotation. A rotation by stem:[\pi] does not give you a reflection._ 

In general a stem:[2 \times 2] unitary matrix can be written as (refer NC p. 20 Box 1.1):

[stem]
++++
\begin{equation}
U = e^{i\alpha} 
     \begin{pmatrix} e^{-i\beta/2} & 0 \\ 0 & e^{+i\beta/2} \end{pmatrix}
     \begin{pmatrix} \cos \frac{\gamma}{2} & -\sin \frac{\gamma}{2} \\ \sin \frac{\gamma}{2} & \cos \frac{\gamma}{2} \end{pmatrix}
     \begin{pmatrix} e^{-i\delta/2} & 0 \\ 0 & e^{+i\delta/2} \end{pmatrix}
\end{equation}
++++

where stem:[\alpha, \beta, \gamma, \delta] are _real_ numbers. The matrix in the middle is a proper rotation matrix. The rest are not strictly rotation matrices but 
depending on who you ask can be understood as "`rotations in a different plane`" (quoting NC). 

Finally, remember that if stem:[H] is Hermitian, then stem:[\exp(iH)] is unitary and if 
stem:[U] is unitary, then stem:[-i\log(U)] is Hermitian. See exercises 2.55, 2.56 on p.84 in Nielsen and Chuang. So behind every unitary matrix
is a Hermitian matrix and vice-versa.

== Vector space vs. Hilbert Space

Books on QM always talk of a Hilbert space. It sounds something very special from a normal vector space. In reality it isn't that special.
A Hilbert space is simply a vector space with a well-defined _inner product_ - that is all there is to it. See NC p. 66.
Technically, a vector space does not have to have a inner product defined on it. Hilbert space is just the more accurate term. 

== Definition: Superposition

When the _probability amplitude vector_ stem:[\Psi] is such that all entries are `0` except for one which is `1`, it is in a 
_pure state_ or a _computational basis state_. This is the case after _measurement_ when all qubits collapse into definite `0` s or `1` s.
In any other case the _probability amplitude vector_ is in a state of _superposition_. i.e., _superposition_ is defined to be the state when 
stem:[\Psi] is not a "`Kronecker delta`" vector.

== Definition: Entanglement

All statements below are equivalent and correspond to the case when the qubits are _not_ in entanglement:

* The joint probability distribution function (pdf) of the _n_ qubits can be expressed as a product of individual pdfs 
* The state of the _i-th_ qubit does not depend on the state of the other qubits
* The _probability amplitude vector_ stem:[\Psi] can be factored out into a tensor product of _n_ sub-vectors as below:

[latexmath#factorizable-pdf]
.Un-entangled qubits or factorizable probability amplitude vector
++++
\begin{equation}
\Psi = \otimes_{i=0}^{i=n} \psi_i 
\end{equation}
++++

_Note carefully that in above equation, the LHS is a vector of length 2^n^ whereas the RHS is a tensor product of n vectors, each vector being of length 2._ 

When <<factorizable-pdf>> does not hold i.e., the joint pdf is not factorizable the qubits are in a state of _entanglement_.

== The Ugly notation

Remember that stem:[|0\rangle] is shorthand for stem:[\begin{pmatrix} 1 \\ 0 \end{pmatrix}] which is the _probability amplitude vector_ of a single qubit
that is in a definite state. And stem:[|1\rangle] is shorthand for stem:[\begin{pmatrix} 0 \\ 1 \end{pmatrix}].

The pure states of a two qubit system are expressed variously as (see https://youtu.be/392t0hBkcwM?t=362[this] for reference)

[options=header]
|===
| Notation 1 | Notation 2 | Tensor Product | Probability Amplitude Vector
| stem:[\|00 \rangle] | stem:[\|0 \rangle \|0 \rangle] | stem:[\|0\rangle \otimes \|0\rangle] | stem:[\begin{pmatrix} 1 \\ 0 \\ 0 \\ 0 \end{pmatrix}]
| stem:[\|01 \rangle] | stem:[\|0 \rangle \|1 \rangle] | stem:[\|0\rangle \otimes \|1\rangle] | stem:[\begin{pmatrix} 0 \\ 1 \\ 0 \\ 0 \end{pmatrix}]
| stem:[\|10 \rangle] | stem:[\|1 \rangle \|0 \rangle] | stem:[\|1\rangle \otimes \|0\rangle] | stem:[\begin{pmatrix} 0 \\ 0 \\ 1 \\ 0 \end{pmatrix}]
| stem:[\|11 \rangle] | stem:[\|1 \rangle \|1 \rangle] | stem:[\|1\rangle \otimes \|1\rangle] | stem:[\begin{pmatrix} 0 \\ 0 \\ 0 \\ 1 \end{pmatrix}]
|===

The _probability amplitude vector_ is rarely written down due to its length. It explodes with _n_. But that's the _real thing_.
You should always remember that when you see stem:[|00\rangle] it is a shorthand for the actual stem:[4 \times 1] vector.

== EPR or Bell Pair

The simplest demonstration of entanglement is with the EPR or Bell Pair which is a two qubit system whose wave function is given by
following where the stem:[\frac{1}{\sqrt 2}] scale factor is removed for brevity:

\begin{equation}
\begin{split}
\Psi & = |00 \rangle + |11 \rangle \\
     & = \begin{pmatrix} 1 \\ 0 \\ 0 \\ 0 \end{pmatrix} + \begin{pmatrix} 0 \\ 0 \\ 0 \\ 1 \end{pmatrix} = \begin{pmatrix} 1 \\ 0 \\ 0 \\ 1 \end{pmatrix}
\end{split}
\end{equation}

Verify that this stem:[4 \times 1] vector cannot be expressed as a tensor product of two stem:[2 \times 1] vectors which is the definition of entanglement. Physically,
if the first qubit is stem:[1] it precludes any possibility that the second qubit can be stem:[0]. Thus, the state of the second qubit is not _independent_
of the state of the first qubit (and vice-versa).

== The Deutsch-Josza Algorithm

Considered the Hello World of quantum computing, I found this a very difficult algorithm to understand.
In fact I don't understand it and the reason for making these notes. Here is the circuit diagram.

image::https://i.stack.imgur.com/SottO.png[link="https://quantumcomputing.stackexchange.com/questions/15253/why-isnt-output-of-deutsch-jozsa-algorithm-simply-0"]

In what follows we consider just 2 qubits or the case when _n=1_ in the diagram above. First of all, let's
understand the notation used in this and other diagrams like this that appear commonly in books etc.
stem:[|\Psi_0 \rangle], stem:[|\Psi_1 \rangle], stem:[|\Psi_2 \rangle] and stem:[|\Psi_3 \rangle] are used to mean the total
_probability amplitude vector_ at the four stages in the circuit. stem:[|\Psi_0 \rangle] is easy:

\begin{equation}
\Psi_0 = |0 \rangle |1 \rangle = |01 \rangle = \begin{pmatrix} 0 \\ 1 \\ 0 \\ 0 \end{pmatrix}
\end{equation}

To get stem:[|\Psi_1 \rangle], we can try to figure out the stem:[4 \times 4] unitary matrix which will transform stem:[|\Psi_0 \rangle] to stem:[|\Psi_1 \rangle].
I have not seen this in any of the books. Rather what they do is to tell the reader to apply the Hadamard transform
individually to the two qubits. Applying Hadamard transform to the stem:[|0 \rangle] qubit gives (stem:[|0 \rangle + |1 \rangle]) (I ignore the scale factor for brevity)
and applying it to stem:[|1 \rangle] qubit gives (stem:[|0 \rangle - |1 \rangle]). stem:[|\Psi_1 \rangle] is then given by the tensor product of these two:

\begin{equation}
\begin{split}
\Psi_1 & = (|0 \rangle + |1 \rangle) \otimes (|0 \rangle - |1 \rangle) \\
       & = |00 \rangle - |01 \rangle + |10 \rangle - |11 \rangle \\
       & = \begin{pmatrix} 1 \\ 0 \\ 0 \\ 0 \end{pmatrix} - \begin{pmatrix} 0 \\ 1 \\ 0 \\ 0 \end{pmatrix} + \begin{pmatrix} 0 \\ 0 \\ 1 \\ 0 \end{pmatrix} - \begin{pmatrix} 0 \\ 0 \\ 0 \\ 1 \end{pmatrix} \\
       & = \begin{pmatrix} 1 \\ -1 \\ 1 \\ -1 \end{pmatrix} 
\end{split}
\end{equation}

The stem:[4 \times 1] vectors on RHS are never written in any textbook but that is what stem:[\Psi_1] _really_ is. It is an equal superposition of all the pure states.

Getting to stem:[\Psi_2] is going to take a lot of work. First, we need to explain what _f_ is. _f_ is a classical scalar - actually boolean - function.
Its input _domain_ is a _classical_ bit string i.e., a number between 0 and 2^n^-1. For the case when _n=1_, its input can be `0` or `1`. For the case when
_n=2_, its input can be `00`, `01`, `10`, `11` or 0, 1, 2, 3 respectively. And its output is a `0` or `1`. This is one of the things I find hard to
understand in this algorithm. _f_ is a classical function but _x_ is not a classical bit. It is a qubit. What is stem:[f(x)] when stem:[x] is in a superposition
of states - it is not even defined. Anyway what the books tell us to do is this - the effect of the stem:[U_f] circuit is to take stem:[|x,y \rangle] and return
stem:[|x,y \oplus f(x) \rangle] and we apply this rule to stem:[\Psi_1] above to give:

\begin{equation}
\Psi_2  = |0,0 \oplus f(0) \rangle  - |0, 1 \oplus f(0) \rangle + |1, 0 \oplus f(1) \rangle - |1, 1 \oplus f(1) \rangle 
\end{equation}

Since stem:[1 \oplus a = \bar a], we get:

\begin{equation}
\Psi_2  = |0, f(0) \rangle  - |0, \bar f(0) \rangle + |1, f(1) \rangle - |1, \bar f(1) \rangle
\end{equation}

This gives following 4 possibilities for stem:[\Psi_2]:

[options=header]
|===
| f(0) | f(1) | stem:[\Psi_2]
| 0 | 0 | stem:[\|00 \rangle  - \|01 \rangle + \|10 \rangle - \|11 \rangle = A] 
| 0 | 1 | stem:[\|00 \rangle  - \|01 \rangle + \|11 \rangle - \|10 \rangle = B] 
| 1 | 0 | stem:[\|01 \rangle  - \|00 \rangle + \|10 \rangle - \|11 \rangle = -B] 
| 1 | 1 | stem:[\|01 \rangle  - \|00 \rangle + \|11 \rangle - \|10 \rangle = A] 
|===

So when _f_ is a constant i.e., stem:[f(0) = f(1)], we have stem:[\Psi_2 = \pm A] (the positive sign is taken when stem:[f(0) = f(1) = 0] and negative sign otherwise)
and when _f_ is balanced i.e., stem:[f(0) \neq f(1)], we have stem:[\Psi_2 = \pm B].

Now to get stem:[\Psi_3] it is convenient to express stem:[\Psi_2] as following tensor product of two qubits so that we can just apply the Hadamard to first qubit to get stem:[\Psi_3]:

\begin{align} \label{A}
A & = (|0 \rangle + |1 \rangle) \otimes (|0 \rangle - |1 \rangle) \\
B & = (|0 \rangle - |1 \rangle) \otimes (|0 \rangle - |1 \rangle)
\end{align}

Now since the Hadmard stem:[H] is its own inverse, applying stem:[H] to (stem:[|0 \rangle + |1 \rangle]) gives back stem:[|0 \rangle] and applying it to 
(stem:[|0 \rangle - |1 \rangle]) gives back stem:[|1 \rangle]. And so stem:[\Psi_3] equals:

\begin{equation}
\Psi_3 = |0 \rangle \otimes (|0 \rangle - |1 \rangle)
\end{equation}

if stem:[f] is constant and

\begin{equation}
\Psi_3 = |1 \rangle \otimes (|0 \rangle - |1 \rangle)
\end{equation}

if stem:[f] is balanced. The first qubit is in a _definite_ state of either stem:[0] or stem:[1] with stem:[100\%] probability.
And measuring the first qubit will tell if stem:[f] is constant or balanced which is the problem the Deutsch-Josza Algorithm is supposed to solve.

I find this algorithm extremely confusing and outright "`wrong`" because by definition the stem:[U_f] gate is supposed to leave the first qubit
unchanged - it maps stem:[|x,y \rangle] to stem:[|x,y \oplus f(x) \rangle] whereas equations above show just the opposite. _The first qubit gets messed up
whereas the second one is unchanged!_ This is my longstanding dilemma with this algorithm. It is contradictory.
Also see https://quantumcomputing.stackexchange.com/questions/15253/why-isnt-output-of-deutsch-jozsa-algorithm-simply-0[this] question on StackExchange.

Let's also see how to get stem:[\Psi_3] using the long method. We apply Hadamard to the first qubit of stem:[A] and stem:[B] expressions.
This gives us following for the case when stem:[\Psi_2 = A]. I am going to drop off all the ugly brakets to simplify notation:

\begin{equation}
\begin{split}
\Psi_3 & = (0 + 1) 0 - (0 + 1) 1 + (0 - 1) 0 - ( 0 - 1) 1 (\textrm{removing brakets to avoid ugly notation}) \\
       & = 00 + 10 - 01 - 11 + 00 - 10 - 01 + 11 \\
       & = 00 - 01 (\textrm{scale factor is not important}) \\
       & = |0 \rangle \otimes (|0 \rangle - |1 \rangle) (\textrm{adding back the brakets})
\end{split}
\end{equation}

which agrees with previous result. Let's also do the exercise for when stem:[\Psi_2 = B]:

\begin{equation}
\begin{split}
\Psi_3 & = (0 + 1) 0 - (0 + 1) 1 + (0 - 1) 1 - ( 0 - 1) 0 \\
       & = 00 + 10 - 01 - 11 + 01 - 11 - 00 + 10 \\
       & = 10 - 11 (\textrm{scale factor is not important}) \\
       & = |1 \rangle \otimes (|0 \rangle - |1 \rangle) (\textrm{adding back the brakets})
\end{split}
\end{equation}

which again agrees with what we obtained previously using the shortcut method. So at least this much is good.

'''

Finally after many months I get it. The problem here is with the way stem:[U_f] circuit is labelled which leads to incorrect understanding of the circuit - at least to me. 
The output of the circuit is stem:[x] unchanged and stem:[y \oplus f(x)] _only when stem:[x] is a classical stem:[n]-bit string._
When stem:[x] is a superposition of states stem:[f(x)] is not even defined. This was the first thing that tripped me when I encountered this circuit.
The way to predict the output of the circuit when its fed a superposition is to use linearity stem:[T(A + B) = T(A) + T(B)]. 
Thus if stem:[x] is a superposition of two states stem:[a] and stem:[b], we need to calculate the output of the circuit to the individual states and then superimpose those outputs
to get the final result. This is exactly what we did above. And when we do that we find the circuit can affect the input superposition.
In particular when stem:[y = |-\rangle], this circuit acts like a _phase-inversion_ (aka _phase-kickback_) circuit and is pervasive in quantum algorithms. 

image::/assets/images/uf_circuit.png[]

To get the last equation, solve for the two cases: case 1 when stem:[f(x) = 0] and case 2 when stem:[f(x) = 1] and you'll see how we get the final formula.
In the last equation, stem:[|-\rangle] is a constant and can be pulled outside the summation sign, _but the rest cannot_. So as a final step what we have is:

[stem]
++++
\begin{equation}
\left(\sum_x (-1)^{f(x)} \alpha_x |x\rangle\right) \otimes |-\rangle
\end{equation}
++++

and variable stem:[x] enumerates the stem:[2^n] classical states. The circuit leaves the stem:[|-\rangle] unchanged and "`messes up`" input superposition. 

In summary, given a quantum circuit stem:[U_f] whose behavior is characterized under classical inputs as:

[stem]
++++
\begin{equation}
|x, y\rangle \xrightarrow{U_f} |x, y \oplus f(x)\rangle
\end{equation}
++++

where:

stem:[x] is a stem:[n] bit classical string,
stem:[y] is a single classical bit, and
stem:[f(\cdot)] is a boolean function

we have shown that:

[stem]
++++
\begin{equation}
\sum_x \alpha_x |x,-\rangle \xrightarrow{U_f} \left(\sum_x (-1)^{f(x)} \alpha_x |x\rangle\right) \otimes |-\rangle
\end{equation}
++++

Watch https://youtu.be/XumGT8Ed84Q?t=111[this] video if you like.

== Quantum Teleportation

The quantum teleportation circuit is shown in:

image::https://miro.medium.com/max/2000/0*97mRZq_jBC8mSOxk.png[link:https://miro.medium.com/max/2000/0*97mRZq_jBC8mSOxk.png]

stem:[\beta_{00}] is the Bell pair stem:[|00 \rangle + |11 \rangle]. Let's do the math:

[latexmath]
++++
\Psi_0 = |\psi \rangle \otimes \left( |00 \rangle + |11 \rangle \right) 
++++

To get stem:[\Psi_1] we have to apply a controlled NOT to the second qubit. So we get following two cases:

[options=header]
|===
| stem:[\psi] | stem:[\Psi_1]
| 0 | 000 + 011
| 1 | 110 + 101
|===

Above is when stem:[\psi] is in a pure state either `0` or `1`. In practice it will be in a quantum state:

[latexmath]
++++
\begin{equation}
\psi = \alpha |0 \rangle + \beta |1 \rangle
\end{equation}
++++

or simply 

[latexmath]
++++
\begin{equation}
\psi = \begin{pmatrix} \alpha \\ \beta \end{pmatrix}
\end{equation}
++++

This means that stem:[\Psi_1] is given by:

[latexmath]
++++
\begin{equation}
\Psi_1 = \alpha (000 + 011) + \beta (110 + 101)
\end{equation}
++++

where I have dropped the ugly brakets for clarity.
Now we have to apply Hadamard to the first qubit giving:

[latexmath]
++++
\begin{equation}
\begin{split}
\Psi_2 & = \alpha \left( (0 + 1) 00 + (0 + 1) 11 \right) + \beta \left( (0 - 1) 10 + (0 - 1) 01 \right) \\
       & = \alpha \left( 000 + 100 + 011 + 111 \right) + \beta \left( 010 - 110 + 001 - 101 \right) \\
       & = \begin{pmatrix} \alpha \\ \beta \\ \beta \\ \alpha \\ \alpha \\ -\beta \\ -\beta \\ \alpha \end{pmatrix} (\textrm{do it as exercise})
\end{split}
\end{equation}
++++

Now we measure the first two qubits. When we do this those qubits will collapse into definite `0` or `1` and we will be left with the wave function
stem:[\psi_3] of just a single qubit. Suppose we find the first two qubits collpase to `00` upon measuring. Then stem:[\Psi_2] collapses to:

[latexmath]
++++
\begin{equation}
\alpha 000 + \beta 001 
\end{equation}
++++

and so stem:[\psi_3] is nothing but stem:[\alpha |0 \rangle + \beta |1 \rangle] or just stem:[\begin{pmatrix} \alpha \\ \beta \end{pmatrix}]. Similarly,
when we do the exercise for the other cases, we end up with following table of results:

[options=header]
|===
| stem:[M_1] | stem:[M_2] | stem:[\psi_3]
| 0 | 0 | \begin{pmatrix} \alpha \\ \beta \end{pmatrix}
| 0 | 1 | \begin{pmatrix} \beta \\ \alpha \end{pmatrix}
| 1 | 0 | \begin{pmatrix} \alpha \\ -\beta \end{pmatrix}
| 1 | 1 | \begin{pmatrix} -\beta \\ \alpha \end{pmatrix}
|===

Voila! In first case, the state stem:[\psi] has been transmitted as-is. And in all the other cases, we can get back stem:[\begin{pmatrix} \alpha \\ \beta \end{pmatrix}]
by applying simple matrix transformations afforded by the stem:[X] and stem:[Z] gates. The stem:[X] gate interchanges (swaps) the amplitudes while the stem:[Z] gate 
negates the second amplitude. The astute reader will notice that applying stem:[XZ] to 
stem:[\begin{pmatrix} -\beta \\ \alpha \end{pmatrix}] gives
stem:[\begin{pmatrix} -\alpha \\ -\beta \end{pmatrix}] which is stem:[-\psi] not stem:[\psi] but this is
inconsequential as quantum states are indistinguishable 
modulo a global phase factor i.e., the state stem:[\psi] cannot be distinguished from stem:[e^{i\theta}\psi].
If you want to get stem:[\psi] you will apply stem:[X] followed by stem:[Z]. 
But the order of the gates doesn't matter. This is _quantum teleportation_. QED.

== The connection between QM and QC

Quantum computers manipulate qubits using unitary matrices, but that is not what we are taught in
physics classes on Quantum Mechanics. QM teaches us the time evolution of a quantum system is described by the Schrodinger equation. How does an equation become a gate?
What gives? This is important concept to understand. See Nielsen and Chuang p. 83 for the connection. The Schrodinger equation is:

[stem]
++++
\begin{equation}
i \hbar \frac{\partial}{\partial t} \Psi = H \Psi
\end{equation}
++++

where I have omitted bra-ket notation. stem:[H], known as Hamiltonian of the system, is an _operator_ that acts on the wavefunction. So its more like
stem:[H(\Psi)]. There are two variables at play here. One is time stem:[t] and another is for example stem:[x] - the position of the particle.
When stem:[x] is a continuous variable, stem:[H] is a differential operator and stem:[\Psi] cannot be written as a vector - or you could write it but it will
have infinite length; hence we hear of infinite dimensional spaces. In quantum computing, we confine ourselves to cases when stem:[x] is not continuous. It is discrete.
In fact, its binary and can take on only two values. An example of such stem:[x] is the spin of an electron. stem:[\Psi] in this case is a stem:[2 \times 1] vector
 - considering a system made up of only one particle. And stem:[H] is a matrix. The time variable in quantum computing is simply the sequence of gates we apply to stem:[\Psi].

Consider a very small duration of time during which we can assume that stem:[H] is constant and unchanging, then the solution to the equation is simply:

[stem]
++++
\begin{equation}
\Psi(t) = \exp (- \frac{i}{\hbar} H t) \Psi(0)
\end{equation}
++++

and so the unitary matrices or gates we encounter in quantum computing are nothing but:

[stem]
++++
\begin{equation}
U = \exp (- \frac{i}{\hbar} H T) 
\end{equation}
++++

where stem:[T] is a tiny constant like the reciprocal of the clock speed of the computer. This is just a metaphor. Quantum computers don't have a clock-rate per se
(refer https://quantumcomputing.stackexchange.com/questions/8441/does-a-quantum-computer-have-a-clock-signal-and-if-yes-how-big-is-it[this]).
If we remove the constants, then stem:[U = \exp(-iH)]. So to apply different gates to the system we need to manipulate its Hamiltonian.

In summary QM can be formulated in a matrix form when the state vector stem:[\Psi] is finite dimensional (i.e., stem:[x] is discrete and takes on only finite number of values).
Quantum computing relies on this formulation of QM and the matrix formulation actually goes all the way back to Werner Heisenberg. See 
https://en.wikipedia.org/wiki/Matrix_mechanics[this] for reference.

== Gate-based computation vs. quantum annealing

D-Wave is a company that makes quantum computers. Its machines have been subject of some controversy e.g., see 
https://quantumcomputing.stackexchange.com/questions/171/is-there-proof-that-the-d-wave-one-is-a-quantum-computer-and-is-effective[this] and references therein.
_D-Wave's machine does not apply gates to a system of qubits._ Instead what it does is a process known as _quantum annealing_.
In this process a system of qubits is prepared in a ground state stem:[H_0] and the Hamiltonian of the system is gradually evolved to a target Hamiltonian stem:[H_1].

[stem]
++++
\begin{equation}
H(t) = (1 - t) \cdot H_0 + t \cdot H_1
\end{equation}
++++

If the process is done slow enough, then there is a https://en.wikipedia.org/wiki/Adiabatic_theorem[theorem] that guarantees that the qubits will remain in their
lowest energy state as the system is evolved. What is the upshot? The upshot is that the target Hamiltonian is such that its ground state will give the solution of 
the problem that we wanted to solve. This is also known as _quantum adiabatic computation_ to distingush it from _gate-based computation_. It has been shown 
(see https://arxiv.org/abs/quant-ph/0405098[this] paper)
that quantum computing by adiabatic evolution is equivalent to unitary gate computation in power:
_anything that a unitary gate quantum computer can do in polynomial time, an adiabatic computer can as well, and vice-versa._
But if you go down the rabbit hole, it seems - I am still learning - quantum annealing (i.e., D-Wave) is not exactly the same as
quantum adiabatic computation. See 5:27 in https://youtu.be/zvfkXjzzYOo?t=327[this] video:

++++
<iframe width="560" height="315" src="https://www.youtube.com/embed/zvfkXjzzYOo" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
++++

and https://quantumcomputing.stackexchange.com/questions/1577/what-is-the-difference-between-quantum-annealing-and-adiabatic-quantum-computati[this] post.

According to https://youtu.be/NwoD82hDImM?t=672[this] video, for a problem to be solved by a quantum annealer, it must be cast into a QUBO - quadratic unconstrained binary
optimization. More on adiabatic quantum computation in https://youtu.be/vFpNNrt-baE[this] video.
