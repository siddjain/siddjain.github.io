---
layout: mathjax-post
title:  "Understanding Observables in Quantum Mechanics"
date:   2020-12-26 14:54:14 -0700
categories: quantum-computing
description: understand what are observables in quantum mechanics
---
= Understanding Observables in Quantum Mechanics
Siddharth Jain <siddjain@live.com>
:revdate: 2020-12-26 10:30:00 -0800
:doctype: article
:stem: latexmath
:eqnums: all
:xrefstyle: short

== Preliminaries 

Recall the stem:[X], stem:[Y] stem:[Z] Pauli Matrices:

[latexmath]
++++
\begin{equation}
X = \begin{pmatrix} 0 && 1 \\ 1 && 0 \end{pmatrix} 
\end{equation}
++++

[latexmath]
++++
\begin{equation}
Y = \begin{pmatrix} 0 && -i \\ i && 0 \end{pmatrix} 
\end{equation}
++++

[latexmath]
++++
\begin{equation}
Z = \begin{pmatrix} 1 && 0 \\ 0 && -1 \end{pmatrix} 
\end{equation}
++++

The eigenvalues of all 3 matrices are stem:[\{+1, -1\}]. Calculate the eigenvectors for yourself.

== Composite Systems

Consider what happens when the transformation stem:[X_1 Z_2] is applied to a two qubit system.
stem:[X] and stem:[Z] are Pauli matrices and the 1 subscript on stem:[X] is used to denote that
the matrix acts on only the first qubit. Similarly for stem:[Z_2]. Lets see how this transformation
will affect the 4 computational basis vectors:

[latexmath]
++++
\begin{equation}
00 \xrightarrow{Z_2} 00 \xrightarrow{X_1} 10 = \begin{pmatrix} 0 && 0 && 1 && 0 \end{pmatrix}^T \\
01 \xrightarrow{Z_2} -01 \xrightarrow{X_1} -11 = \begin{pmatrix} 0 && 0 && 0 && -1 \end{pmatrix}^T \\
10 \xrightarrow{Z_2} 10 \xrightarrow{X_1} 00 = \begin{pmatrix} 1 && 0 && 0 && 0 \end{pmatrix}^T \\
11 \xrightarrow{Z_2} -11 \xrightarrow{X_1} -01 = \begin{pmatrix} 0 && -1 && 0 && 0 \end{pmatrix}^T \\
\end{equation}
++++

The matrix corresponding to the transformation stem:[X_1 Z_2] is thus given by stacking the column
vectors as follows:

[latexmath]
++++
\begin{equation}
M = \begin{pmatrix}
0 && 0 && 1 && 0 \\
0 && 0 && 0 && -1 \\
1 && 0 && 0 && 0 \\
0 && -1 && 0 && 0
\end{pmatrix}
\end{equation}
++++

Note that this matrix is equal to the tensor product of the stem:[X] and stem:[Z] matrices:

[latexmath]
++++
\begin{equation}
M = \begin{pmatrix}
0 && 1 \\
1 && 0 
\end{pmatrix} \otimes
\begin{pmatrix}
1 && 0 \\
0 && -1 
\end{pmatrix}
\end{equation}
++++

Suppose we have to find the matrix corresponding to stem:[X_2 Z_1]. Repeating the procedure above
we will get 

[latexmath]
++++
\begin{equation}
00 \xrightarrow{Z_1} 00 \xrightarrow{X_2} 01 = \begin{pmatrix} 0 && 1 && 0 && 0 \end{pmatrix}^T \\
01 \xrightarrow{Z_1} 01 \xrightarrow{X_2} 00 = \begin{pmatrix} 1 && 0 && 0 && 0 \end{pmatrix}^T \\
10 \xrightarrow{Z_1} -10 \xrightarrow{X_2} -11 = \begin{pmatrix} 0 && 0 && 0 && -1 \end{pmatrix}^T \\
11 \xrightarrow{Z_1} -11 \xrightarrow{X_2} -10 = \begin{pmatrix} 0 && 0 && -1 && 0 \end{pmatrix}^T \\
\end{equation}
++++

Thus

++++
\begin{equation}
M = \begin{pmatrix}
0 && 1 && 0 && 0 \\
1 && 0 && 0 && 0 \\
0 && 0 && 0 && -1 \\
0 && 0 && -1 && 0
\end{pmatrix}
\end{equation}
++++

and we can check that this is same as the tensor product of stem:[Z] and stem:[X] matrices:

[latexmath]
++++
\begin{equation}
M = \begin{pmatrix}
1 && 0 \\
0 && -1 
\end{pmatrix} \otimes
\begin{pmatrix}
0 && 1 \\
1 && 0 
\end{pmatrix}
\end{equation}
++++

== Observations

Lets go back to the stem:[X_1 Z_2] matrix and now understand what happens when a measurement
is made after applying this matrix. The matrix is referred to as an _observable_ in this case.
An _observable_ is constrained to be a _Hermitian_ operator:

[latexmath]
.Definition of Hermitian (aka self-adjoint) operator
++++
\begin{equation}
M = M ^ \dagger
\end{equation}
++++

where stem:[\dagger] denotes the _adjoint_ which
is formed by taking element-wise complex conjugate followed by transpose operation which turns
column vectors into row vectors. Note that measurement operator need not be unitary.

What does the measurement process do? It does two things:

* _The measurement process collapses the wavefunction to one of the orthonormal eigen vectors
(also known as eigen states) of the matrix stem:[M]_. 
* _The result of measurement is an eigenvalue of stem:[M]. That is what the measurement device registers._

The collapse is given by:

[latexmath#collapse]
.Wavefunction collapse onto the i-th eigenvector stem:[\textbf{q}_i]
++++
\begin{equation}
\Psi^{'} = e^{i\theta} \textbf{q}_i
\end{equation}
++++

where stem:[e^{i\theta}] is given by projecting the wavefunction
onto stem:[\textbf{q}_i] and normalizing the length of the resulting complex number. i.e., let

[latexmath#projection]
.Projection of the wavefunction onto the i-th eigenvector stem:[\textbf{q}_i]
++++
\begin{equation}
z_i = \textbf{q}_i ^ \dagger \Psi
\end{equation}
++++

Then stem:[e^{i\theta}] is simply stem:[\frac{z_i}{||z_i||}] and the probability with which the
wavefunction will collapse to stem:[\textbf{q}_i] is given by

[latexmath#eq1]
.Probability of measuring _i-th_ eigenvalue
++++
\begin{equation}
p_i = ||z_i||^2
\end{equation}
++++

The above equation is also written as follows in the brakets notation and is the same thing:

[latexmath]
++++
\begin{equation}
p_i = \langle\Psi|\textbf{q}_i\rangle\langle\textbf{q}_i|\Psi\rangle
\end{equation}
++++

By default, when we see circuit diagrams with a measurement gate, it is measuring with
stem:[M=Z] where stem:[Z] is the Pauli stem:[Z] matrix with eigen states stem:[|0\rangle]
and stem:[|1\rangle]. This is also known as _measurement in the computational basis_.

_What if we have repeated eigenvalues? What eigenvector does the wavefunction collapse to?_
In this case the wavefunction will collapse to the Hilbert space spanned by the associated eigenvectors.
Umesh Vazirani talks about this in https://youtu.be/Aw-8NDnNG-8?t=501[this] video.

== An Example

Lets do the math to make it clear taking stem:[M = X_1 Z_2] as example. First, we need to calculate the
eigenvalues and eigenvectors of the matrix stem:[M]. Do that as exercise. The eigenvalues are given by:

[latexmath]
++++
\begin{equation}
\lambda = \{ +1, -1, +1, -1 \} (\textrm{there are duplicate eigenvalues})
\end{equation}
++++

and the _orthonormal_ eigenvectors are given by columns of the following matrix:

[latexmath]
++++
Q = \begin{equation}
\frac{1}{\sqrt 2} \begin{pmatrix}
1 &&  1 &&  0 &&  0 \\
0 &&  0 && -1 &&  1 \\
1 && -1 &&  0 &&  0 \\
0 &&  0 &&  1 &&  1 
\end{pmatrix}
\end{equation}
++++

Verify:

[latexmath]
++++
\begin{equation}
\begin{pmatrix}
0 && 0 && 1 && 0 \\
0 && 0 && 0 && -1 \\
1 && 0 && 0 && 0 \\
0 && -1 && 0 && 0
\end{pmatrix}
\begin{pmatrix}
1 && 1 && 0 && 0 \\
0 &&  0 && -1 &&  1 \\
1 && -1 &&  0 &&  0 \\
0 &&  0 &&  1 &&  1 
\end{pmatrix}
\end{equation}
 = \begin{pmatrix}
1 && -1 && 0 && 0 \\
0 &&  0 && -1 &&  -1 \\
1 &&  1 &&  0 &&   0 \\
0 &&  0 &&  1 &&  -1 
\end{pmatrix}
++++

Above is nothing but:

[latexmath]
++++
\begin{equation}
M Q = Q \Lambda
\end{equation}
++++

As exercise we can take stem:[\Psi] to be stem:[\frac{1}{\sqrt 2}(|00 \rangle + |11 \rangle)]. 

For _i = 0_:

[latexmath]
++++
\begin{equation}
\textbf{v}_0 \textbf{v}_0 ^ \dagger = \frac{1}{2} \begin{pmatrix} 1 \\ 0 \\ 1 \\ 0 \end{pmatrix}
\begin{pmatrix} 1 && 0 && 1 && 0 \end{pmatrix} = \frac{1}{2} \begin{pmatrix} 1 && 0 && 1 && 0 \\
0 && 0 && 0 && 0 \\
1 && 0 && 1 && 0 \\  
0 && 0 && 0 && 0
\end{pmatrix}
\end{equation}
++++

and we can check that

[latexmath]
++++
\begin{equation}
p_0 = \Psi ^ \dagger \textbf{v}_0 \textbf{v}_0 ^ \dagger \Psi = \frac{1}{4}
\end{equation}
++++

If you do the math you will find that stem:[p_1, p_2, p_3] are all stem:[\frac{1}{4}].

The _average_ value of the observable is:

[latexmath]
++++
\begin{equation}
\langle M \rangle = \sum_i \lambda_i p_i
\end{equation}
++++

and turns out to be same as:

[latexmath]
++++
\begin{equation}
\langle M \rangle = \Psi ^ \dagger M \Psi
\end{equation}
++++

which in this case will be 0. The best book I have found on QM is not a book but the
https://inst.eecs.berkeley.edu/~cs191/fa10/[Lecture Notes] by Umesh Vazirani. 
https://www.youtube.com/watch?v=VPsl_5RQe1A&list=PLnhoxwUZN7-6hB2iWNhLrakuODLaxPTOG[Here] are videos of his lectures.
This is a must watch for anyone. 
